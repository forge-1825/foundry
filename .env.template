# System Configuration (Core application settings)
HOST_IP=localhost # Host IP address
FRONTEND_PORT=3456 # Frontend web server port (mapped to 80 in container)
BACKEND_PORT=7433 # Backend API server port
API_KEY=your-api-key # API key for backend
VLLM_API_KEY=your-vllm-api-key # API key for VLLM models
CORS_ORIGINS=http://localhost:${FRONTEND_PORT:-3456} # CORS allowed origins
DATA_DIR=./data # Data directory path
SCRIPTS_DIR=./scripts # Scripts directory path
LOG_LEVEL=INFO # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
ENABLE_GPU=0 # Enable GPU support (0 or 1)

# Frontend Configuration
REACT_APP_API_URL=http://${HOST_IP:-localhost}:${BACKEND_PORT:-7433} # Backend API URL
REACT_APP_API_KEY=${API_KEY}
REACT_APP_WS_URL=ws://localhost:${BACKEND_PORT:-7433}
REACT_APP_ENV=development
REACT_APP_DEBUG=true

# Backend SSH Configuration
# SSH_KEY_FILENAME=id_rsa_docker_remote # The name of your private key file in the build context

USE_REMOTE_MODELS=True # Use remote model containers (true/false)

# Remote Model Docker Container Host
# Must open set docker daemon to listen on port 2375 or whatever configured port
SSH_TUNNEL_DOCKER_HOST=tcp://host.docker.internal:2375 #change variable name to something more descriptive
DOCKER_API_VERSION=1.41

#  This is the ssh command to forward the local machine ports to the remote 
#  server's docker daemon to access remotely running docker containers
#  replace the address with the address of the machine running remote containers
# and the user with the username of the remote machine

# ssh -L 2375:localhost:2375 -L 8000:localhost:8000 -L 8001:localhost:8001 user@remote-address -Nf
# N : dont start a shell or run any commands, just keep connection
# f : run in background

# Teacher Model Configuration (Model serving)
TEACHER_MODEL_NAME=Teacher # Model name
TEACHER_MODEL_PORT=8000 # Port number
TEACHER_MODEL_HOST=host.docker.internal # Remote model server IP
TEACHER_MODEL_ID=microsoft/Phi-3-mini-4k-instruct-AWQ # Model ID (from Hugging Face)
TEACHER_MODEL_PATH=/models/teacher # Path to teacher model files (must contain model files)
TEACHER_MODEL_REMOTE=True # Enable remote connection
TEACHER_MODEL_API_KEY=not-needed # API key for remote teacher model

# Student Model Configuration (Model serving)
STUDENT_MODEL_NAME=Student # Model name
STUDENT_MODEL_PORT=8001 # Port number
STUDENT_MODEL_HOST=host.docker.internal # Remote model server IP
STUDENT_MODEL_ID=microsoft/Phi-3-mini-4k-instruct-AWQ # Model ID (from Hugging Face)
STUDENT_MODEL_PATH=/models/student # Path to student model files (must contain model files)
STUDENT_MODEL_REMOTE=True # Enable remote connection
STUDENT_MODEL_API_KEY=not-needed # API key for remote student model

# Distilled Model Configuration (Model serving)
DISTILLED_MODEL_NAME=Distilled
DISTILLED_MODEL_PORT=8003 # Port Number
DISTILLED_MODEL_HOST=host.docker.internal # Remote model server IP
DISTILLED_MODEL_ID=microsoft/Phi-3-mini-4k-instruct-AWQ
DISTILLED_MODEL_PATH=/models/distilled
DISTILLED_MODEL_REMOTE=True # Enable remote connection
DISTILLED_MODEL_API_KEY=not-needed # API key for remote distilled model

# Novelty Insights Configuration (for novelty tracking feature)
CURIOSITY_LOG_PATH=curiosity.log
NOVELTY_COUNTS_PATH=novelty_counts.json
LEARNING_LOG_PATH=learning_log_metasploit.json

# Python Environment
PYTHONUNBUFFERED=1 # Ensure Python output is not buffered